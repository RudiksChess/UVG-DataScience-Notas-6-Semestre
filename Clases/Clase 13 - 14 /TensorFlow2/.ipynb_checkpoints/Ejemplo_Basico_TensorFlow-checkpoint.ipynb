{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejemplo básico con TensorFlow 2.0\n",
    "\n",
    "En este ejercicio vamos a recrear nuestro algoritmo de aprendizaje utilizando TF 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importar las librerías relevantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.6.0-cp37-cp37m-macosx_10_11_x86_64.whl (198.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 198.9 MB 36 kB/s  eta 0:00:012    |███████████████████▏            | 118.9 MB 5.0 MB/s eta 0:00:16\n",
      "\u001b[?25hCollecting numpy~=1.19.2\n",
      "  Downloading numpy-1.19.5-cp37-cp37m-macosx_10_9_x86_64.whl (15.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 15.6 MB 9.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions~=3.7.4 in /Users/rudiks/opt/anaconda3/lib/python3.7/site-packages (from tensorflow) (3.7.4.3)\n",
      "Collecting absl-py~=0.10\n",
      "  Downloading absl_py-0.13.0-py3-none-any.whl (132 kB)\n",
      "\u001b[K     |████████████████████████████████| 132 kB 8.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting keras~=2.6\n",
      "  Downloading keras-2.6.0-py2.py3-none-any.whl (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 7.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting keras-preprocessing~=1.1.2\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "\u001b[K     |████████████████████████████████| 42 kB 2.7 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wrapt~=1.12.1 in /Users/rudiks/opt/anaconda3/lib/python3.7/site-packages (from tensorflow) (1.12.1)\n",
      "Collecting protobuf>=3.9.2\n",
      "  Downloading protobuf-3.17.3-cp37-cp37m-macosx_10_9_x86_64.whl (1.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.0 MB 296 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting h5py~=3.1.0\n",
      "  Downloading h5py-3.1.0-cp37-cp37m-macosx_10_9_x86_64.whl (2.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.9 MB 964 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting gast==0.4.0\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting termcolor~=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Collecting astunparse~=1.6.3\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting tensorboard~=2.6\n",
      "  Downloading tensorboard-2.6.0-py3-none-any.whl (5.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.6 MB 4.1 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting clang~=5.0\n",
      "  Downloading clang-5.0.tar.gz (30 kB)\n",
      "Collecting tensorflow-estimator~=2.6\n",
      "  Downloading tensorflow_estimator-2.6.0-py2.py3-none-any.whl (462 kB)\n",
      "\u001b[K     |████████████████████████████████| 462 kB 4.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting grpcio<2.0,>=1.37.0\n",
      "  Downloading grpcio-1.39.0-cp37-cp37m-macosx_10_10_x86_64.whl (3.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.9 MB 5.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting opt-einsum~=3.3.0\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[K     |████████████████████████████████| 65 kB 888 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-pasta~=0.2\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[K     |████████████████████████████████| 57 kB 4.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six~=1.15.0 in /Users/rudiks/opt/anaconda3/lib/python3.7/site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: wheel~=0.35 in /Users/rudiks/opt/anaconda3/lib/python3.7/site-packages (from tensorflow) (0.36.2)\n",
      "Collecting flatbuffers~=1.12.0\n",
      "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Collecting cached-property\n",
      "  Downloading cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-macosx_10_9_x86_64.whl (3.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.5 MB 5.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.0-py3-none-any.whl (781 kB)\n",
      "\u001b[K     |████████████████████████████████| 781 kB 7.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: werkzeug>=0.11.15 in /Users/rudiks/opt/anaconda3/lib/python3.7/site-packages (from tensorboard~=2.6->tensorflow) (1.0.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/rudiks/opt/anaconda3/lib/python3.7/site-packages (from tensorboard~=2.6->tensorflow) (2.25.1)\n",
      "Collecting google-auth<2,>=1.6.3\n",
      "  Downloading google_auth-1.35.0-py2.py3-none-any.whl (152 kB)\n",
      "\u001b[K     |████████████████████████████████| 152 kB 6.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.4-py3-none-any.whl (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 6.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools>=41.0.0 in /Users/rudiks/opt/anaconda3/lib/python3.7/site-packages (from tensorboard~=2.6->tensorflow) (52.0.0.post20210125)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "\u001b[K     |████████████████████████████████| 155 kB 3.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.2.2-py3-none-any.whl (11 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.7.2-py3-none-any.whl (34 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: importlib-metadata in /Users/rudiks/opt/anaconda3/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow) (3.10.0)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "\u001b[K     |████████████████████████████████| 77 kB 5.6 MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: chardet<5,>=3.0.2 in /Users/rudiks/opt/anaconda3/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/rudiks/opt/anaconda3/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (1.26.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/rudiks/opt/anaconda3/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/rudiks/opt/anaconda3/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2021.5.30)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.1.1-py2.py3-none-any.whl (146 kB)\n",
      "\u001b[K     |████████████████████████████████| 146 kB 6.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /Users/rudiks/opt/anaconda3/lib/python3.7/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.6->tensorflow) (3.4.1)\n",
      "Building wheels for collected packages: clang, termcolor\n",
      "  Building wheel for clang (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for clang: filename=clang-5.0-py3-none-any.whl size=30702 sha256=cf601cca2ce639b3333072e903c30fa17663fb9c79387fabbd8f597f62a00419\n",
      "  Stored in directory: /Users/rudiks/Library/Caches/pip/wheels/98/91/04/971b4c587cf47ae952b108949b46926f426c02832d120a082a\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4829 sha256=ed391c855ac03e405e956861bacada97c98e21f128a1c4aab533b11edf0ab3fd\n",
      "  Stored in directory: /Users/rudiks/Library/Caches/pip/wheels/3f/e3/ec/8a8336ff196023622fbcb36de0c5a5c218cbb24111d1d4c7f2\n",
      "Successfully built clang termcolor\n",
      "Installing collected packages: pyasn1, rsa, pyasn1-modules, oauthlib, cachetools, requests-oauthlib, google-auth, tensorboard-plugin-wit, tensorboard-data-server, protobuf, numpy, markdown, grpcio, google-auth-oauthlib, cached-property, absl-py, termcolor, tensorflow-estimator, tensorboard, opt-einsum, keras-preprocessing, keras, h5py, google-pasta, gast, flatbuffers, clang, astunparse, tensorflow\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.20.2\n",
      "    Uninstalling numpy-1.20.2:\n",
      "      Successfully uninstalled numpy-1.20.2\n",
      "  Attempting uninstall: h5py\n",
      "    Found existing installation: h5py 2.10.0\n",
      "    Uninstalling h5py-2.10.0:\n",
      "      Successfully uninstalled h5py-2.10.0\n",
      "Successfully installed absl-py-0.13.0 astunparse-1.6.3 cached-property-1.5.2 cachetools-4.2.2 clang-5.0 flatbuffers-1.12 gast-0.4.0 google-auth-1.35.0 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.39.0 h5py-3.1.0 keras-2.6.0 keras-preprocessing-1.1.2 markdown-3.3.4 numpy-1.19.5 oauthlib-3.1.1 opt-einsum-3.3.0 protobuf-3.17.3 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.0 rsa-4.7.2 tensorboard-2.6.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.0 tensorflow-2.6.0 tensorflow-estimator-2.6.0 termcolor-1.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generacióm de datos\n",
    "\n",
    "Generaremos los datos de la misma manera que lo hicimos en el ejercicio annterior.  La única diferencia es que ahora guardamos los datos en un archivo *.npz*.  NPZ es el tipo de archivo propio de NumPy que permite guardar los arreglos NumPy.  Introducimos este cambio porque en el ML a menudo:\n",
    "\n",
    "* nos dan datos (csv, base de datos, etc.)\n",
    "* preprocesamos los datos y los dejamos en un formato deseado (veremos este tema después)\n",
    "* se guardan los datos en archivos npz (si es que estamos trabajando con Python) para uso posterior\n",
    "\n",
    "No hay nada especial de todo esto.  Solo guardamos arreglos NumPy en un archivo reutlizable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trabajaremos con dos variables de entrada, las x1 y x2. Se generan al azar a partir de una distribución uniforme.\n",
    "\n",
    "Se creará una matriz con estas dos variables.  La matriz X del modelo lineal y = x * w + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Por facilidad, declaramos una variable que indique el tamaño del conjunto \n",
    "#      de datos de entrenamiento.\n",
    "observaciones = 100000\n",
    "\n",
    "x1 = np.random.uniform(low=-10, high=10, size=(observaciones,1))\n",
    "x2 = np.random.uniform(-10, 10, (observaciones,1))\n",
    "\n",
    "entradas_generadas = np.column_stack((x1,x2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generar las metas a las que debemos apuntar\n",
    "\n",
    "Inventaremos una función f(x1, x2) = 2 * x1 - 3 * x2 + 5 + <ruido pequeño>.  El ruido es para hacerlo más realista.\n",
    "\n",
    "En esencia estamos diciendo que los pesos serán 2 y -3, y es sesgo es 5\n",
    "\n",
    "Utilizaremos la metodología de ML, y veremos si el algoritmo la ha aprendido. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruido = np.random.uniform(-1, 1, (observaciones,1))\n",
    "\n",
    "targets_generados = 2 * x1 - 3 * x2 + 5 + ruido"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hasta ahora todo ha sido igual.  El siguiente paso sí es nuevo y es que estaremos guardando la información en un archivo *.npz* que llamaremos \"Datos_TF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('Datos_TF', entradas = entradas_generadas, targets = targets_generados)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resolver con TensorFlow\n",
    "\n",
    "<i/>Nota: Esta introducción de TensorFlow es muy básica.  El TF tiene muchas más capacidaded y profundidad que esto.<i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a cargar los datos desde el archivo NPZ.  Por supuesto, esto no era necesario acá\n",
    "datos_entrenamiento = np.load('Datos_TF.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el ejercicio anterior tuvimos que dar valores iniciales, acá solo damos los tamaños"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tamanio_entrada = 2   # el número de variables que tenemos\n",
    "\n",
    "tamanio_salida = 1   # el número de salidas que tenemos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delineamos o esbozamos el modelo.  \n",
    "\n",
    "Se hace con **\"Sequential\"**\n",
    "\n",
    "Notese que no se pide cálculo alguno - solo describimos nuestra red\n",
    "\n",
    "Acá se debe listar cada capa \"layer\"\n",
    "\n",
    "El método **\"Dense\"** indica, que nuestra operación matemática será (xw + b)\n",
    "\n",
    "Hay otros parámetros que se pueden incluir para particularizar el modelo, en nuestro caso solo estamos tratando de crear una solución que sea tan parecida a la de nuestro modelo NumPy-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "                           \n",
    "                            tf.keras.layers.Dense(tamanio_salida,\n",
    "                                                 kernel_initializer=tf.random_uniform_initializer(minval=-0.1, maxval=0.1),\n",
    "                                                 bias_initializer=tf.random_uniform_initializer(minval=-0.1, maxval=0.1)\n",
    "                                                 )\n",
    "                            ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También podemos definir un optimizador a la medida, donde podemos especificar la tasa de aprendizaje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizador_adhoc = tf.keras.optimizers.SGD(learning_rate=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es posible que se necesite una función de pérdida a la medida.  Eso es mucho más difícil implementar y no trabajaremos esto en el curso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**\"compile\"** es donde podemos seleccionar e indicar los optimizadores y la pérdida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = optimizador_adhoc, loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente ajustamos el modelo, indicando las entradas y los targets.  \n",
    "\n",
    "En vez de usar el término *iteraciones*, se utiliza el término *épocas*.  Si no se especifica el número de épocas este será 1 (una sola época de entrenamiento), así que este número es algo obligatorio.\n",
    "\n",
    "El parámetro **\"verbose\"** se refiere a cuánta información queremos que despliegue durante la ejecución.  Se vale probar diferentes números...uno que es bastante bueno es el 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(datos_entrenamiento['entradas'], datos_entrenamiento['targets'], epochs = 100, verbose = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Extracción de los pesos y sesgos\n",
    "\n",
    "La exctracción de el(los) peso(s) y sesgo(s) de un modelo no es un paso esencial para el proceso de ML.  De hecho, en un contexto de aprendizaje profundo, no nos daría mucha información útil.  Sin embargo este ejemplo simple se armó de tal forma que nos permite verificar si las respueestas que obtenemos son correctas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[0].get_weights()    # el cero (0) es porque solo tenemos una capa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos almacenar los pesos y los sesgos en variables diferentes para facilitar la revisión.\n",
    "\n",
    "OJO!   Pueden haber cientos o miles de estos!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pesos = model.layers[0].get_weights()[0]\n",
    "pesos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sesgos = model.layers[0].get_weights()[1]\n",
    "sesgos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraer las salidas y hacer predicciones\n",
    "\n",
    "Una vez más, este no es un paso esencial, sin embargo, generalmente vamos a querer hacer predicciones.\n",
    "\n",
    "Podemos predecir nuevos valores para hacer uso del modelo.  A veces es útil redondear los valores para que la salida sea más legible.  \n",
    "\n",
    "Generalmente se utiliza este método con DATOS NUEVOS, en vez de usar los datos de entrenamiento originales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict_on_batch(datos_entrenamiento['entradas']).round(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si desplegamos nuestras metas (valores reales), podemos compararlas manualmente con las predicciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos_entrenamiento['targets'].round(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graficar los datos\n",
    "\n",
    "El modelo está ya optimizado, por lo que las salidas se han calculado sobre la última forma, o estado, del modelo.\n",
    "\n",
    "Necesitamos comprimir o empacar **\"squeeze\"** los arreglos para dejarlos en un formato que es el esperado por la función graficadora.  No cambia nada ya que dejamos dimensiones de tamaño 1 - solo es un tecnisismo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.squeeze(model.predict_on_batch(datos_entrenamiento['entradas'])), \n",
    "         np.squeeze(datos_entrenamiento['targets']))\n",
    "plt.xlabel('salidas')\n",
    "plt.ylabel('targets')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(x = np.squeeze(model.predict_on_batch(datos_entrenamiento['entradas'])), \n",
    "                 y =  np.squeeze(datos_entrenamiento['targets']))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Comparación predicciones vrs metas\",\n",
    "    xaxis_title=\"Salidas (Predicciones)\",\n",
    "    yaxis_title=\"Targets (Metas)\",\n",
    "    width = 600,\n",
    "    height = 400,)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Listo, lo que vemos debe ser exactamente igual a lo que vimos en el ejercicio pasado!\n",
    "\n",
    "A estas alturas quizás no le vean la gracia al TensorFlow.  En términos de líneas código es igual al del ejercicio con NumPy para llegar al mismo resultado.  Sin embargo, a medida que profundizemos en el tema, veremos que TensorFlow nos ahorrará cientos de líneas de código."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
