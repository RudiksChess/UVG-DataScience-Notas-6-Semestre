{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejemplo básico con TensorFlow 2.0\n",
    "\n",
    "En este ejercicio vamos a recrear nuestro algoritmo de aprendizaje utilizando TF 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importar las librerías relevantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generacióm de datos\n",
    "\n",
    "Generarémos los datos de la misma manera que lo hicimos en el ejercicio annterior.  La única diferencia es que ahora guardamos los datos en un archivo *.npz*.  NPZ es el tipo de archivo propio de NumPy que permite guardar los arreglos NumPy.  Introducimos este cambio porque en el ML a menudo:\n",
    "\n",
    "* nos dan datos (csv, base de datos, etc.)\n",
    "* preprocesamos los datos y los dejamos en un formato deseado (veremos este tema después)\n",
    "* se guardan los datos en archivos npz (si es que estamos trabajando con Python) para uso posterior\n",
    "\n",
    "No hay nada especial de todo esto.  Solo guardamos arreglos NumPy en un archivo re-utlizable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trabajaremos con dos variables de entrada, las x1 y x2. Se generan al azar a partir de una distribución uniforme.\n",
    "\n",
    "Se creará una matriz con estas dos variables.  La matriz X del modelo lineal y = x * w + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Por facilidad, declaramos una variable que indique el tamaño del conjunto \n",
    "#      de datos de entrenamiento.\n",
    "observaciones = 1000\n",
    "\n",
    "x1 = np.random.uniform(low=-10, high=10, size=(observaciones,1))\n",
    "x2 = np.random.uniform(-10, 10, (observaciones,1))\n",
    "\n",
    "entradas_generadas = np.column_stack((x1,x2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generar las metas a las que debemos apuntar\n",
    "\n",
    "Inventaremos una función f(x1, x2) = 2 * x1 - 3 * x2 + 5 + <ruido pequeño>.  El ruido es para hacerlo más realista.\n",
    "\n",
    "En esencia estamos diciendo que los pesos serán 2 y -3, y es sesgo es 5\n",
    "\n",
    "Utilizaremos la metodología de ML, y veremos si el algoritmo la ha aprendido. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruido = np.random.uniform(-1, 1, (observaciones,1))\n",
    "\n",
    "targets_generados = 2 * x1 - 3 * x2 + 5 + ruido"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hasta ahora todo ha sido igual.  El siguiente paso sí es nuevo y es que estaremos guardando la información en un archivo *.npz* que llamaremos \"Datos_TF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('Datos_TF', entradas = entradas_generadas, targets = targets_generados)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resolver con TensorFlow\n",
    "\n",
    "<i/>Nota: Esta introducción de TensorFloe es muy básica.  El TF tiene muchas más capacidaded y profundidad que esto.<i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a cargar los datos desde el archivo NPZ.  Por supuesto, esto no era necesario acá\n",
    "datos_entrenamiento = np.load('Datos_TF.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el ejercicio anterior tuvimos que dar valores iniciales, acá solo damos los tamaños"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tamanio_entrada = 2   # el número de variables que tenemos\n",
    "\n",
    "tamanio_salida = 1   # el número de salidas que tenemos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delineamos o esbozamos el modelo.  \n",
    "\n",
    "Se hace con **\"Sequential\"**\n",
    "\n",
    "Notese que no se pide cálculo alguno - solo describimos nuestra red\n",
    "\n",
    "Acá se debe listar cada capa \"layer\"\n",
    "\n",
    "El método **\"Dense\"** indica, que nuestra operación matemática será (xw + b)\n",
    "\n",
    "Hay otros parámetros que se pueden incluir para particularizar el modelo, en nuestro caso solo estamos tratando de crear una solución que sea tan parecida a la de nuestro modelo NumPy-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "                           \n",
    "                            tf.keras.layers.Dense(tamanio_salida,\n",
    "                                                 kernel_initializer=tf.random_uniform_initializer(minval=-0.1, maxval=0.1),\n",
    "                                                 bias_initializer=tf.random_uniform_initializer(minval=-0.1, maxval=0.1)\n",
    "                                                 )\n",
    "                            ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También podemos definir un optimizador a la medida, donde podemos especificar la tasa de aprendizaje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizador_adhoc = tf.keras.optimizers.SGD(learning_rate=0.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es posible que se necesite una función de pérdida a la medida.  Eso es mucho más difícil implementar y no trabajaremos esto en el curso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**\"compile\"** es donde podemos seleccionar e indicar los optimizadores y la pérdida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = optimizador_adhoc, loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente ajustamos el modelo, indicando las entradas y los targets.  \n",
    "\n",
    "En vez de usar el término *iteraciones*, se utiliza el término *épocas*.  Si no se especifica el número de épocas este será 1 (una sola época de entrenamiento), así que este número es algo obligatorio.\n",
    "\n",
    "El parámetro **\"verbose\"** se refiere a cuánta información queremos que despliegue durante la ejecución.  Se vale probar diferentes números...uno que es bastante bueno es el 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(datos_entrenamiento['entradas'], datos_entrenamiento['targets'], epochs = 100, verbose = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Extracción de los pesos y sesgos\n",
    "\n",
    "La exctracción de el(los) peso(s) y sesgo(s) de un modelo no es un paso esencial para el proceso de ML.  De hecho, en un contexto de aprendizaje profundo, no nos daría mucha información útil.  Sin embargo este ejemplo simple se armó de tal forma que nos permite verificar si las respueestas que obtenemos son correctas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[0].get_weights()    # el cero (0) es porque solo tenemos una capa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos almacenar los pesos y los sesgos en variables diferentes para facilitar la revisión.\n",
    "\n",
    "OJO!   Pueden haber cientos o miles de estos!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pesos = model.layers[0].get_weights()[0]\n",
    "pesos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sesgos = model.layers[0].get_weights()[1]\n",
    "sesgos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraer las salidas y hacer predicciones\n",
    "\n",
    "Una vez más, este no es un paso esencial, sin embargo, generalmente vamos a querer hacer predicciones.\n",
    "\n",
    "Podemos predecir nuevos valores para hacer uso del modelo.  A veces es útil redondear los valores para que la salida sea más legible.  \n",
    "\n",
    "Generalmente se utiliza este método con DATOS NUEVOS, en vez de usar los datos de entrenamiento originales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict_on_batch(datos_entrenamiento['entradas']).round(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si desplegamos nuestras metas (valores reales), podemos compararlas manualmente con las predicciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos_entrenamiento['targets'].round(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graficar los datos\n",
    "\n",
    "El modelo está ya optimizado, por lo que las salidas se han calculado sobre la última forma, o estado, del modelo.\n",
    "\n",
    "Necesitamos comprimir o empacar **\"squeeze\"** los arreglos para dejarlos en un formato que es el esperado por la función graficadora.  No cambia nada ya que dejamos dimensiones de tamaño 1 - solo es un tecnisismo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.squeeze(model.predict_on_batch(datos_entrenamiento['entradas'])), \n",
    "         np.squeeze(datos_entrenamiento['targets']))\n",
    "plt.xlabel('salidas')\n",
    "plt.ylabel('targets')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(x = np.squeeze(model.predict_on_batch(datos_entrenamiento['entradas'])), \n",
    "                 y =  np.squeeze(datos_entrenamiento['targets']))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Comparación predicciones vrs metas\",\n",
    "    xaxis_title=\"Salidas (Predicciones)\",\n",
    "    yaxis_title=\"Targets (Metas)\",\n",
    "    width = 600,\n",
    "    height = 400,)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Listo, lo que vemos debe ser exactamente igual a lo que vimos en el ejercicio pasado!\n",
    "\n",
    "A estas alturas quizás no le vean la gracia al TensorFlow.  En términos de líneas código es igual al del ejercicio con NumPy para llegar al mismo resultado.  Sin embargo, a medida que profundizemos en el tema, veremos que TensorFlow nos ahorrará cientos de líneas de código."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
