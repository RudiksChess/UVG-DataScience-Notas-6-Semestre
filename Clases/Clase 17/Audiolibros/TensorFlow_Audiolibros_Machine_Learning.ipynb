{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejemplo práctico. Audiolibros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problema\n",
    "\n",
    "Nos han dado datos obtenidos con una App para Audiolibros.  Lógicamente, tiene que ver únicamente con las versiones audio de libros.  Cada cliente en la base de datos ha hecho una compra por lo menos una vez, por eso está en la base de datos.  Deseamos crear un algoritmo de ML que nos permita predecir si un cliente volverá a hacer una compra a la empresa de Audiolibros.\n",
    "\n",
    "La idea principal es que si un cliente tiene una baja probabilidad de regresar, no hay razón alguna para gastar dinero en hacerles anunicios a él/ella.  Si podemos enfocar nuestros esfuerzos SOLAMENTE en aquellos clientes que probablemente compren de nuevo, podemos hacer grandes ahorros.  Es más, este modelo puede identificar las métricas importantes para que un cliente regrese.  La identificación de nuevos clientes crea valor y oportunidades de crecimiento.\n",
    "\n",
    "Se tiene un .csv que resume los datos.  Hay varias variables:\n",
    "\n",
    "* Customer ID, \n",
    "* Book length overall (suma de la duración en minutos de todas las compras), \n",
    "* Book length avg (duración promedio, en minutos, de todas las compras), \n",
    "* Price paid_overall (suma del precio de todas las compras),\n",
    "* Price Paid avg (precio promedo de todas las compras), \n",
    "* Review (una variable Booleana indicando si el cliente dejó una evaluación), \n",
    "* Review out of 10 (si el/la cliente dejó una evaluación, su calificación de 0 a 10), \n",
    "* Total minutes listened (total de minutos escuchados), \n",
    "* Completion (completación de 0 a 1), \n",
    "* Support requests (número de requerimientos de soporte técnico), y \n",
    "* Last visited minus purchase date (Ultima visita menos la fecha de compra, en días)\n",
    "\n",
    "Estas son las entradas (excluyendo Customer ID, ya que este es completamente arbitrario.  Es más como un nombre que un número).\n",
    "\n",
    "Las metas son una variable Booleana (0 ó 1).  Tomaremos un período de 2 años para nuestras entradas, y los siguientes 6 meses como metas.  De esta forma, en realidad estamos prediciendo si:  basado en los últimos 2 años de actividad e involucramiento, un cliente volverá a comprar dentro de los siguientes 6 meses.  Seis meses suena con un tiempo razonable.  Si no compran luego de 6 meses, lo más probable es que se han ido a la competencia o no les gustó el formato de Audiolibros para recibir información.\n",
    "\n",
    "La tarea es simple:  crear un algoritmo de ML, que pueda predecir si un cliente volverá a comprar.\n",
    "\n",
    "Este es un problema de clasificación con dos clases:  \"no comprará\" o \"comprará\", representado por 0s y 1s. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crear el algoritmo de ML\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importar las librerías relevantes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datos\n",
    "\n",
    "Se cargarán los datos almacenados luego del pre-procesamiento (ver Notebook aparte)\n",
    "\n",
    "Se crea una variable temporal *npz* para cargar los archivos, y que luego se utilizará para separar las entradas y metas de cada subconjunto.  Para esto se utilizará la palabra clave que se usó para guardarlos \"entradas\" o \"metas\".\n",
    "\n",
    "Se asegurará que las entradas sean de tipo float y las metas de tipo int.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "npz = np.load('Audiolibros_datos_entreno.npz')\n",
    "entradas_entreno = npz['entradas'].astype(np.float)\n",
    "metas_entreno = npz['metas'].astype(np.int)\n",
    "\n",
    "npz = np.load('Audiolibros_datos_validacion.npz')\n",
    "entradas_validacion, metas_validacion = npz['entradas'].astype(np.float), npz['metas'].astype(np.int)\n",
    "\n",
    "npz = np.load('Audiolibros_datos_prueba.npz')\n",
    "entradas_prueba, metas_prueba = npz['entradas'].astype(np.float), npz['metas'].astype(np.int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo\n",
    "\n",
    "Bosquejo, optimizadores, pérdida, detención temprana y entrenamiento\n",
    "\n",
    "De lo anterior, se ha visto todo menos lo de detención (parada) temprana.  Este es un mecanismo para detener las iteraciones, una vez se detecta que la pérdida de validación empieza a aumentar.  \n",
    "\n",
    "Se hace a través del método \"callback\" de **keras**.  El método callback permite ejecutar una rutina al completar un proceso, en este caso usaremos el **EarlyStopping** que detiene las iteraciones al detectar un aumento en la pérdida de validación.  El default es al primer aumento.  Sin embargo, se puede configurar con el argumento **patience**, que representa el número consecutivo de aumentos que debe detener el proceso.  Como es posible que hayan variaciones aleatorias en la pérdida, se usará un valor de 2...cuando haya dos aumentos consecutivos se detiene la iteración.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3579 samples, validate on 447 samples\n",
      "Epoch 1/100\n",
      "3579/3579 - 0s - loss: 0.5406 - accuracy: 0.7868 - val_loss: 0.4206 - val_accuracy: 0.8479\n",
      "Epoch 2/100\n",
      "3579/3579 - 0s - loss: 0.3729 - accuracy: 0.8720 - val_loss: 0.3309 - val_accuracy: 0.8680\n",
      "Epoch 3/100\n",
      "3579/3579 - 0s - loss: 0.3252 - accuracy: 0.8824 - val_loss: 0.3018 - val_accuracy: 0.8904\n",
      "Epoch 4/100\n",
      "3579/3579 - 0s - loss: 0.3036 - accuracy: 0.8863 - val_loss: 0.2882 - val_accuracy: 0.8881\n",
      "Epoch 5/100\n",
      "3579/3579 - 0s - loss: 0.2902 - accuracy: 0.8908 - val_loss: 0.2800 - val_accuracy: 0.8993\n",
      "Epoch 6/100\n",
      "3579/3579 - 0s - loss: 0.2813 - accuracy: 0.8961 - val_loss: 0.2663 - val_accuracy: 0.9016\n",
      "Epoch 7/100\n",
      "3579/3579 - 0s - loss: 0.2706 - accuracy: 0.9019 - val_loss: 0.2619 - val_accuracy: 0.8971\n",
      "Epoch 8/100\n",
      "3579/3579 - 0s - loss: 0.2652 - accuracy: 0.9008 - val_loss: 0.2587 - val_accuracy: 0.9060\n",
      "Epoch 9/100\n",
      "3579/3579 - 0s - loss: 0.2612 - accuracy: 0.9014 - val_loss: 0.2567 - val_accuracy: 0.9016\n",
      "Epoch 10/100\n",
      "3579/3579 - 0s - loss: 0.2562 - accuracy: 0.9036 - val_loss: 0.2577 - val_accuracy: 0.9038\n",
      "Epoch 11/100\n",
      "3579/3579 - 0s - loss: 0.2525 - accuracy: 0.9056 - val_loss: 0.2507 - val_accuracy: 0.9083\n",
      "Epoch 12/100\n",
      "3579/3579 - 0s - loss: 0.2481 - accuracy: 0.9044 - val_loss: 0.2494 - val_accuracy: 0.9105\n",
      "Epoch 13/100\n",
      "3579/3579 - 0s - loss: 0.2462 - accuracy: 0.9098 - val_loss: 0.2610 - val_accuracy: 0.8949\n",
      "Epoch 14/100\n",
      "3579/3579 - 0s - loss: 0.2453 - accuracy: 0.9072 - val_loss: 0.2480 - val_accuracy: 0.9016\n",
      "Epoch 15/100\n",
      "3579/3579 - 0s - loss: 0.2477 - accuracy: 0.9089 - val_loss: 0.2455 - val_accuracy: 0.9038\n",
      "Epoch 16/100\n",
      "3579/3579 - 0s - loss: 0.2423 - accuracy: 0.9092 - val_loss: 0.2459 - val_accuracy: 0.9172\n",
      "Epoch 17/100\n",
      "3579/3579 - 0s - loss: 0.2399 - accuracy: 0.9098 - val_loss: 0.2403 - val_accuracy: 0.9150\n",
      "Epoch 18/100\n",
      "3579/3579 - 0s - loss: 0.2378 - accuracy: 0.9114 - val_loss: 0.2472 - val_accuracy: 0.9016\n",
      "Epoch 19/100\n",
      "3579/3579 - 0s - loss: 0.2366 - accuracy: 0.9120 - val_loss: 0.2437 - val_accuracy: 0.9083\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1b9933850b8>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tamanio_entrada = 10\n",
    "tamanio_salida = 2\n",
    "tamanio_capa_escondida = 50\n",
    "    \n",
    "modelo = tf.keras.Sequential([\n",
    "\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 1era capa escondida\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 2nda capa escondida\n",
    "   \n",
    "    tf.keras.layers.Dense(output_size, activation='softmax') # capa salida\n",
    "])\n",
    "\n",
    "\n",
    "modelo.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "tamanio_tanda = 100\n",
    "\n",
    "epocas_max = 100\n",
    "\n",
    "\n",
    "parada_temprana = tf.keras.callbacks.EarlyStopping(patience=2)\n",
    "\n",
    "\n",
    "modelo.fit(entradas_entreno,\n",
    "          metas_entreno, \n",
    "          batch_size = tamanio_tanda,\n",
    "          epochs = epocas_max, # asumiendo que no entra en acción la detención temprana\n",
    "          callbacks=[parada_temprana], \n",
    "          validation_data=(entradas_validacion, metas_validacion), \n",
    "          verbose = 2 \n",
    "          )  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probar el modelo\n",
    "\n",
    "Luego de entrenar el modelo con los datos de entrenamiento, y validar con los datos de validación, debemos probar el potencial predictivo final del modelo.  Esto se hace ejecutando el modelo con los datos de prueba, que el modelo NUNCA ha visto.\n",
    "\n",
    "Es importante recordar que experimentar con los hiperparámetros causa un sobreajuste de los datos de validación.\n",
    "\n",
    "La prueba es la instancia final absoluta.  No debe llevarse a cabo la prueba antes de haber terminado con el ajuste del modelo.  Si se hacen ajustes luego de la prueba, se empieza a sobreajustar con el conjunto de datos de prueba.  Esto \"tira por la borda\" el propósito de la prueba. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "448/448 [==============================] - 0s 41us/sample - loss: 0.2391 - accuracy: 0.9107\n"
     ]
    }
   ],
   "source": [
    "perdida_prueba, precision_prueba = modelo.evaluate(entradas_prueba, metas_prueba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test loss: 0.24. Test accuracy: 91.07%\n"
     ]
    }
   ],
   "source": [
    "print('\\nPérdida de prueba: {0:.2f}. Precisión de prueba: {1:.2f}%'.format(perdida_prueba, precision_prueba * 100.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizando el modelo e hiperparámetros iniciales dados en este Notebook, la precisión final de la prueba debe rondar en un 91%.\n",
    "\n",
    "Cada vez que se ejecuta el código, se obtiene una precisión distinta porque cada entrenamiento es distinto.\n",
    "\n",
    "Intencionalmente se ha dejado en una solución sub-óptima para que se pueda tratar de edificar sobre la misma."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
